<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Tuning - Rauthy Documentation</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href=".././mdbook-admonish.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Rauthy Documentation</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="tuning"><a class="header" href="#tuning">Tuning</a></h1>
<p>Rauthys default tuning is optimized for somewhat low resource usage while still providing good performance and low
latency up to a couple of thousands of users. If you run a tiny instance with less than 100 users, or a very big one
with users going into millions, or you need a very high degree of concurrency, you can optimize the default tuning.
We will go through a few options to achieve what you need, no matter if you want even lower memory usage or higher
throughput.</p>
<h2 id="database"><a class="header" href="#database">Database</a></h2>
<h3 id="cache_storage_disk"><a class="header" href="#cache_storage_disk"><code>cache_storage_disk</code></a></h3>
<p>By default, Rauthy keeps the WAL + Snapshots for the in-memory cache on disk. This makes the cache, even though it
exists in-memory only, persistent, because it can be rebuilt from disk after a restart. Blacklisted IPs for instance
exists in the cache only, as well as Auth Codes and some other values. The other advantage is, that it reduces the
overall memory usage a bit, because we don't need duplicate data in-memory just to make the Raft replication work.</p>
<p>These advantages are really nice and usually, because the throughput is really high anyway, you should not need to
worry about it. However, you can optionally keep everything cache related in memory only. This boosts the throughput
a lot, depending on the IOPS of your disk of course, at the cost of higher memory usage and the fact, that nodes need
to re-join the Raft cluster after a restart and do a complete re-sync for latest Snapshot + Raft Logs.</p>
<p>To keep everything in memory only, set:</p>
<pre><code class="language-toml">[cluster]
# Set to `false` to store Cache WAL files + Snapshots in-memory only.
# If you run a Cluster, a Node can re-sync cache data after a restart.
# However, if you restart too quickly or shut down the whole cluster,
# all your cached data will be gone.
# In-memory only hugegly increases the throughput though, so it
# depends on your needs, what you should prefer.
#
# default: true
# overwritten by: HQL_CACHE_STORAGE_DISK
cache_storage_disk = false
</code></pre>
<h3 id="read_pool_size--pg_max_conn"><a class="header" href="#read_pool_size--pg_max_conn"><code>read_pool_size</code> / <code>pg_max_conn</code></a></h3>
<p>These values define the database connection pool sizes. <code>pg_max_conn</code> is by default set to <code>20</code>, and
<code>read_pool_size</code> to <code>4</code>.</p>
<p>The Postgres connections are a limit on the maximum and the application will dynamically open as many as necessary. If
you have a huge amount of concurrency, you might see improvements by increasing <code>pg_max_conn</code>. Keep in mind though, that
most data that is used for normal user authentication is cached in memory, so it's possible that it will not help at
all. If you want to lower resource usage as much as possible, you might want to consider a lower value for
<code>pg_max_conn</code>, even though I would only do this for very tiny Rauthy instances with less than 50 users. The connection
pool size for Postgres has a much higher impact than it does for Hiqlite, because of the networking overhead.</p>
<p>For Hiqlite with <code>read_pool_size</code>, you might want to do the same of course. However, the pool is statically built to
reduce latency. On the other hand, a higher amount of connections will only very slightly increase memory usage. These
connections are only for reading data and don't do anything regarding writes. The Hiqlite writer will always be a fixed,
single writer connection running on a dedicated thread.</p>
<p>The Hiqlite read pool can be configured in the <code>cluster</code> section:</p>
<pre><code class="language-toml">[cluster]
# The size of the pooled connections for local database reads.
#
# Do not confuse this with a pool size for network databases, as it
# is much more efficient. You can't really translate between them,
# because it depends on many things, but assuming a factor of 10 is
# a good start. This means, if you needed a (read) pool size of 40
# connections for something like a postgres before, you should start
# at a `read_pool_size` of 4.
#
# Keep in mind that this pool is only used for reads and writes will
# travel through the Raft and have their own dedicated connection.
#
# default: 4
# overwritten by: HQL_READ_POOL_SIZE
#read_pool_size = 4
</code></pre>
<p>The <code>pg_max_conn</code> is located in the <code>database</code> section:</p>
<pre><code class="language-toml">[database]
# Max DB connections for the Postgres pool.
#
# default: 20
# overwritten by: PG_MAX_CONN
pg_max_conn = 20
</code></pre>
<h3 id="log_sync"><a class="header" href="#log_sync"><code>log_sync</code></a></h3>
<p>When using Hiqlite, you can control the Raft Log sync / flush strategy. This has an immediate effect on disk longevity
and crash resistance, but of course also on total throughput.</p>
<pre><code class="language-toml">[cluster]
# Setting for Raft Log syncing / flushing.
#
# This value is probably the most important, depending on your needs.
# It has a huge impact on both performance and consistency.
#
# You can set 3 different levels:
# - immediate
# - immediate_async
# - interval_&lt;time_ms&gt; (e.g. interval_200)
#
# If you run a single instance "Cluster", you most probably want
# `immediate` to have the highest degree of consistency. If set
# to `immediate`, the Raft will block until data has been flushed
# to disk. This is especially important for a single instance
# deployment, because there is no way to recover state from other
# nodes or re-sync a maybe corrupted WAL file.
# `immediate` has a very huge negative impact on throughput, and
# it puts a lot of stress on your disk, which is important to
# consider for SSDs.
#
# `immediate_async` puts the same amount of stress on your SSD
# and flushed all buffers to disk immediately after a single
# Raft Log was saved. Unlike `immediate` however, it does not
# wait for completion and directly returns `success` to the
# Raft Engine. You have a bit less consistency guarantees in
# exchange for basically the same throughput as with `interval`
# syncing.
#
# The `interval_&lt;ms&gt;` option will not flush immediately. Flushes
# will be triggered by an external ticker task top flush buffers
# every &lt;ms&gt; ms, and then only if buffers are dirty, meaning if
# any data has been updated since the last sync. This setting
# has the highest throughput and puts the lowest amount of stress
# on your SSD.
# CAUTION: You probably never want to use this setting for a
# single instance, since it may be the case that if you are
# unlucky and your app crashes before buffers are synced, that
# your WAL file might end up being corrupted. Even though very
# unlikely in real world (can be force-produced though), you
# would need to re-sync from a healthy cluster member in such
# a case, if the automactic WAL repair does not succeed.
#
# default: immediate_async
# overwritten by: HQL_LOG_SYNC
log_sync = "immediate_async"
</code></pre>
<h2 id="wal_size--logs_until_snapshot"><a class="header" href="#wal_size--logs_until_snapshot"><code>wal_size</code> + <code>logs_until_snapshot</code></a></h2>
<p>This applies to Hiqlite only as well. If you only run a small instance, you could save a little bit of memory by
adjusting the WAL file size and the threshold when to create a new Snapshot and cleanup Raft Logs. The default values
will handle as much throughput as your disk IOPS allow. However, you could tune it down for small instance to e.g.</p>
<ul>
<li><code>wal_size = 262144</code></li>
<li><code>logs_until_snapshot = 1000</code></li>
</ul>
<pre><code class="language-toml">[cluster]
# Hiqlite WAL files (when not using the `rocksdb` feature) will
# always have a fixed size, even when they are still "empty", to
# reduce disk operations while writing. You can set the WAL size
# in bytes. The default value is 2 MB, while the minimum size is
# 8 kB.
#
# default: 2097152
# overwritten by: HQL_WAL_SIZE
wal_size = 2097152

# Sets the limit when the Raft will trigger the creation of a new
# state machine snapshot and purge all logs that are included in
# the snapshot.
# Higher values can achieve more throughput in very write heavy
# situations but will end up in more disk usage and longer
# snapshot creations / log purges.
#
# default: 10000
# overwritten by: HQL_LOGS_UNTIL_SNAPSHOT
logs_until_snapshot = 10000
</code></pre>
<p>This will trigger more often WAL log roll-overs, which of course does more syscalls. But since WAL files are
memory-mapped, you can save a little bit of memory at runtime. These values should always be adjusted together. They
both have an impact on how many WAL files will exist during normal operation. ideally, you have 2-4 WAL files around
during normal operation. At the same time, you should make make these values too small so that Snapshots are not being
created more than probably every 15 minutes.</p>
<div id="admonition-note" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-title">
<div class="admonition-title">
<div id="admonition-note-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="#admonition-note"></a>
</div>
<div>
<p>You should not push the <code>wal_size</code> below <code>131072</code>. The benefits in memory usage will become very small in exchange for
a lot more disk I/O. Also, the <code>wal_size</code> should always be a multiple of <code>4096</code> for optimal alignment. Values bigger
than the default 2MiB will typically not have any positive impact as well, as long as you are not having too many
Snapshots because of a huge amount of concurrent users.</p>
</div>
</div>
<h2 id="http-server"><a class="header" href="#http-server">HTTP Server</a></h2>
<h3 id="http_workers"><a class="header" href="#http_workers"><code>http_workers</code></a></h3>
<p>The <code>server.http_workers</code> variable defines the amount of HTTP worker threads being spawned. This has a big impact on
the amount of memory used. This is especially important when you run Rauthy inside a container on a bigger underlying
host with many cores. Rauthy will spawn more workers depending on the total amount of cores, and not depending on any
container limits you might have set, which it cannot see.</p>
<p>The default value is:</p>
<ul>
<li>less than 4 CPU cores -&gt; 1</li>
<li>4+ cores -&gt; max(2, cores - MAX_HASH_THREADS - reserve), where <code>reserve</code> is 2 when <code>HIQLITE=true</code> and 1 otherwise</li>
</ul>
<p>You almost <strong>always want to tune this</strong> to your needs, it has a big impact on memory! The best idea is probably to use
the same formula, but with respect to custom container limits, or whatever you would want Rauthy to use.</p>
<pre><code class="language-toml">[server]
# Limits the amount of HTTP worker threads. This value
# heavily impacts memory usage, even in idle. The default
# values are:
# - less than 4 CPU cores -&gt; 1
# - 4+ cores -&gt; max(2, cores - MAX_HASH_THREADS - reserve)
#   where `reserve` is 2 when `HIQLITE=true` and 1 otherwise.
#
# CAUTION: If you run your instance on a big underlying host,
# you almost always want to manually set an appropriate
# value. Rauthy can only see all available cores and not any
# possibly set container limits. This means if it runs inside
# a container on something like a 96 core host, Rauthy will
# by default spawn very many threads.
#
# overwritten by: HTTP_WORKERS
http_workers = 1
</code></pre>
<h3 id="metrics_enable"><a class="header" href="#metrics_enable"><code>metrics_enable</code></a></h3>
<p>In newer versions, the <code>metrics_enable</code> is <code>false</code> by default and opt-in. You can enable prometheus metrics with it. An
independent HTTP server will be spawned, it will consume additional memory and CPU. Leave it to <code>false</code> if not needed.</p>
<pre><code class="language-toml">[server]
# To enable or disable the additional HTTP server to expose
# the /metrics endpoint.
#
# default: false
# overwritten by: METRICS_ENABLE
metrics_enable = true
</code></pre>
<h3 id="swagger_ui_enable"><a class="header" href="#swagger_ui_enable"><code>swagger_ui_enable</code></a></h3>
<p>If enable <code>swagger_ui_enable</code> and set it to <code>true</code>, it will consume ~13 MB of additional memory. To reduce overall
memory fragmentation further down the road, if enabled, it will be initialized at the very start of the application
instead of being lazily initialized. If you don't need the API documentation, leave it to the default, which is <code>false</code>.</p>
<pre><code class="language-toml">[server]
# Can be set to `true` to enable the Swagger UI.
# This will consume ~13mb of additional memory.
#
# default: false
# overwritten by: SWAGGER_UI_ENABLE
swagger_ui_enable = true
</code></pre>
<h2 id="password-hashing"><a class="header" href="#password-hashing">Password Hashing</a></h2>
<h3 id="argon2_m_cost--argon2_t_cost--argon2_p_cost--max_hash_threads"><a class="header" href="#argon2_m_cost--argon2_t_cost--argon2_p_cost--max_hash_threads"><code>argon2_m_cost</code> + <code>argon2_t_cost</code> + <code>argon2_p_cost</code> + <code>max_hash_threads</code></a></h3>
<p>Values for password hashing are probably the most important ones to tune overall. They do not only dictate resource
usage, but also the strength and security of password hashes. The password hashing will always be the limiting factor,
at least as long as not all of your users use Webauthn-only accounts. This book, as well as the Admin UI, have dedicated
sections and utilities for these values. Just keep in mind to tune them properly.</p>
<pre><code class="language-toml">[hashing]
# Argon2ID hashing parameters. Take a look at the documentation
# for more information:
# https://sebadob.github.io/rauthy/config/argon2.html
# M_COST must never be below 32768 in production
# overwritten by: ARGON2_M_COST
argon2_m_cost = 131072
# T_COST must never be below 1 in production
# overwritten by: ARGON2_T_COST
argon2_t_cost = 4
# P_COST must never be below 2 in production
# overwritten by: ARGON2_P_COST
argon2_p_cost = 8

# Limits the maximum amount of parallel password hashes at the exact same time
# to never exceed system memory while still allowing a good amount of memory
# for the Argon2ID algorithm
#
# CAUTION: You must make sure, that you have at least
# (MAX_HASH_THREADS * ARGON2_M_COST / 1024) + idle memory of your deployment available.
#
# default: 2
# overwritten by: MAX_HASH_THREADS
max_hash_threads = 2
</code></pre>
<h2 id="memory-allocator"><a class="header" href="#memory-allocator">Memory Allocator</a></h2>
<p>Rauthy uses <code>jemalloc</code> under the hood, which you can tune via an environment variable, as long as it's set before the
application starts. A default setting is baked into the compiled binary, but when you provide the env var, you can
overwrite it. The default is tuned to be just fine for instances with a couple of hundreds of users and focuses more
on being efficient rather than providing a high degree of concurrency.</p>
<p>If you have a bigger instance with many thousands or even millions of users, or just a tiny one with probably less than
100, you can optimize it. There is only a small amount of room in terms of lowering memory usage, but a lot of
increasing concurrency at the cost of higher memory usage.</p>
<p>I will not go into the details of <code>jemalloc</code> here. If you are interested in this topic, you will find lots of
information via the search engine of your choice. I only want to give you recipes that should just work.</p>
<div id="admonition-caution" class="admonition admonish-warning" role="note" aria-labelledby="admonition-caution-title">
<div class="admonition-title">
<div id="admonition-caution-title">
<p>Caution</p>
</div>
<a class="admonition-anchor-link" href="#admonition-caution"></a>
</div>
<div>
<p>You need to set the <code>MALLOC_CONF</code> environment variable from the recipes below BEFORE the application starts, or
otherwise it will be ignored. Anything like the <code>-e</code> flag for docker, or a regular environment variable inside K8s will
work.</p>
</div>
</div>
<h3 id="small-instance"><a class="header" href="#small-instance">Small Instance</a></h3>
<p>If you run a small instance with less than 100 users and you want to reduce the memory footprint as much as possible,
you have two options.</p>
<p><strong>1. Lowest memory usage</strong></p>
<p>This value will probably give you no noticeable performance hit while still providing a low memory usage.</p>
<pre><code>MALLOC_CONF=abort_conf:true,narenas:1,tcache_max:1024,dirty_decay_ms:1000,muzzy_decay_ms:1000
</code></pre>
<p><strong>2. Lowest possible memory at all costs.</strong></p>
<p>This will come with a <strong>very big performance hit</strong>! You should only use it, if you don't care about performance at all.
Imho, the performance hit is not worth the small savings compared to the config above.</p>
<pre><code>MALLOC_CONF=abort_conf:true,narenas:1,tcache:false,dirty_decay_ms:0,muzzy_decay_ms:0
</code></pre>
<h3 id="medium-instance"><a class="header" href="#medium-instance">Medium Instance</a></h3>
<p>If you run a medium instance with up to 500-1000 users, you are probably just fine with the defaults. Stick to them.
Just for reference, the currently used default config is this:</p>
<pre><code>MALLOC_CONF=abort_conf:true,narenas:8,tcache_max:4096,dirty_decay_ms:5000,muzzy_decay_ms:5000
</code></pre>
<h3 id="big-instance"><a class="header" href="#big-instance">Big Instance</a></h3>
<p>A big instance is one with more than at least 1000 users. In this case, you might not get the desired performance,
especially if you have a higher value for <code>MAX_HASH_THREADS</code> or <code>HTTP_WORKERS</code>. Most requests are so small, that they
almost entirely fit in the thread-local cache for the allocator, but if dynamic brotli compression kicks in, memory
from an allocator arena will be necessary most of the time. Because Rauthy does not need the arena's that often, the
default value of <code>4</code> is too low in such a case. You should set <code>narenas</code> from the string below at least equal to the
amount of CPU cores your Rauthy instance is assigned to. Depending on the amount or concurrent logins, you may set
<code>narenas</code> to 2-4x CPU cores at the cost of higher memory usage.</p>
<pre><code>MALLOC_CONF=abort_conf:true,narenas:16,tcache_max:16384,dirty_decay_ms:10000,muzzy_decay_ms:10000
</code></pre>
<h3 id="open-end"><a class="header" href="#open-end">Open End</a></h3>
<p>For any instance with basically an open end of users, or if you have a very high degree of concurrent logins (or you
just don't care about low memory usage and only about max performance), set <code>narenas</code> to 4x your CPU cores (higher
values will start to have a negative impact). This config will let you scale into millions of users easily.</p>
<pre><code>MALLOC_CONF=abort_conf:true,narenas:64,tcache_max:32768,dirty_decay_ms:30000,muzzy_decay_ms:30000
</code></pre>
<div id="admonition-note-1" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-1-title">
<div class="admonition-title">
<div id="admonition-note-1-title">
<p>Note</p>
</div>
<a class="admonition-anchor-link" href="#admonition-note-1"></a>
</div>
<div>
<p>The Memory Allocator tuning does not work on Windows msvc targets, if you are running a custom-compiled version of
Rauthy. It should work everywhere else though.</p>
</div>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../config/unix_socket.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../auth_providers/index.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../config/unix_socket.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../auth_providers/index.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
